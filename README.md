# awesome-attention
A curated list of papers on attention mechanism for machine learning. Papers are ranking by publication date.

See also [another curated list on the subject](https://github.com/Separius/awesome-fast-attention).

# For natural language processing

## Structured attention networks
ICLR 2017. 
Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush

[ArXiv](https://arxiv.org/abs/1702.00887) -- [Code](https://github.com/harvardnlp/struct-attn)

## Latent Alignment and Variational Attention
NIPS 2018. Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander M. Rush

[ArXiv](https://arxiv.org/abs/1807.03756) -- [Code](https://github.com/harvardnlp/var-attn)

## Differentiable Dynamic Programming for Structured Prediction and Attention
ICML 2018. Arthur Mensh, Matthieu Blondel

[ArXiv](https://arxiv.org/abs/1802.03676) -- [Code](https://github.com/arthurmensch/didypro0)

## Not All Attention Is Needed: Gated Attention Network for Sequence Data
AAAI 2020. Lanqing Xue, Xiaopeng Li, Nevin L. Zhang

[ArXiv](https://arxiv.org/abs/1912.00349)



# For computer vision
